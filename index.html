<!DOCTYPE html>
<html lang="de">

<head>
  <!--=================N ECESSITIES AND BOOTSTRAP==================-->

  <meta charset="utf-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" type="text/css" href="util/css/bootstrap.min.css">
  <!-- Latest compiled and minified JavaScript -->
  <!-- Get my own stylesheet in here -->
  <link rel="stylesheet" type="text/css" href="util/css/main.css">
  <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous"> -->
  <!-- <script src="/static/vendor/jquery/3.5.1/jquery-3.5.1.min.js"></script> -->
  <!-- <script src="/static/vendor/bootstrap/5.0.0-beta1-dist/js/bootstrap.bundle.min.js" integrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW" crossorigin="anonymous"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>
  <!--===================M ETA INFO========================-->

  <title>Leonie Weissweiler</title>

  <link rel="icon" href="util/uu.svg" type="image/svg+xml">




  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file://
        -->
  <!--[if lt IE 9]>
        <![endif]-->
        <style>
        body {
            position: relative;
        }
        #show-more-btn {
        background-color: #f8f9fa; /* Light gray */
        border-color: #ddd;       /* Border to match the light gray */
        color: #333;              /* Darker text color for readability */
      }

    </style>
</head>

<body>

  <div class="container">
  <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light">
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">
        <a class="nav-link" href="#">Home</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#scrollspyHeading1">News</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#">Publications</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#">Talks</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="Teaching">Teaching</a>
      </li>
    </ul>
  </div>
</nav> -->
    <div class="col-sm-10">
      <!--==================PRETTY HEADER=====================-->

      <h1>Leonie Weissweiler</h1><h2> <p class="text-secondary" id="ipa">/‚ÄãÀàl‚ÄãeÀê‚Äão‚Äãn‚Äãi ‚ÄãÀàva…™ÃØ‚Äãsva…™ÃØ‚Äã‚Äãl‚Äã‚Äã…ê/</p></h2>


      <div class="text-justify">

        <div id="top-links">
          <a href="mailto:weissweiler@utexas.edu"><img src="util/images/email.png" height="24"></a>
          <a href="https://x.com/LAWeissweiler" target="_blank"><img src="util/images/X.png" height="24"></a>
          <a href="https://bsky.app/profile/weissweiler.bsky.social" target="_blank"><img src="util/images/bluesky.png" height="24"></a>
          <a href="https://scholar.google.com/citations?user=o4fK4n4AAAAJ&amp;hl=en" target="_blank"><img src="util/images/scholar.png" height="40"></a>  
          <a href="https://github.com/LeonieWeissweiler" target="_blank"><img src="util/images/octocat.png" height="24"></a>
        </div>

        <div style="overflow:hidden;">
        <div style="float:right; margin:0 0 5px 20px;"><img src="util/images/profile.jpg" class="img-thumbnail" style="height: 200px;" /></div>

          I am a postdoc at <a href="https://www.uu.se/en/department/linguistics-and-philology/research/computational-linguistics">Uppsala University Computational Linguistics</a>, working with <a href="https://jnivre.github.io/">Joakim Nivre</a> and funded by the <a href="https://www.dfg.de/en/research-funding/funding-opportunities/programmes/individual/walter-benjamin">Walter Benjamin Fellowship</a> of the <a href="https://www.dfg.de/en">German Research Foundation (DFG)</a>. <br><br>
          
          Before that, I was a postdoc at <a href="https://liberalarts.utexas.edu/linguistics/">UT Austin Linguistics</a>, working with <a href="https://mahowak.github.io/">Kyle Mahowald</a>, funded by the <a href="https://www.dfg.de/en/research-funding/funding-opportunities/programmes/individual/walter-benjamin">German Research Foundation (DFG)</a> and the <a href="https://www2.daad.de/ausland/studieren/stipendium/de/70-stipendien-finden-und-bewerben/?detail=57243862">German Academic Exchange Service (DAAD)</a>.
          I completed my PhD in 2024 at the <a href="https://www.cis.uni-muenchen.de">Center for Information and Language Processing</a> at <a href="https://www.en.uni-muenchen.de/index.html">LMU Munich</a> where my thesis was about <i>Computational Approaches to Construction Grammar and Morphology</i>. My supervisor was <a href="https://schuetze.cis.lmu.de/">Hinrich Sch√ºtze</a>.
          Previously, I completed my B.Sc. and M.Sc. degrees in Computational Linguistics and Computer Science at LMU, with scholarships from the <a href="https://www.studienstiftung.de/en/">German Academic Scholarship Foundation</a> and the <a href="https://www.elitenetzwerk.bayern.de/en/home/funding-programs/max-weber-program">Max Weber Program</a>.
          My M.Sc. thesis, supervised by Hinrich Sch√ºtze, was on the application of Complementary Learning Systems Theory to NLP.
          I spent the final year of my bachelor's degree as a visiting student at <a href="https://www.homerton.cam.ac.uk/">Homerton College</a>, <a href="https://www.cam.ac.uk/">University of Cambridge</a>, where I wrote my B.Sc. thesis on Character-Level RNNs under the supervision of <a href="https://sites.google.com/site/annakorhonen/">Anna Korhonen</a>.<br><br>

          <br><b>Current Research Interests</b>
          <div class="entry-content">
            <ul>
              <li>Construction Grammar and NLP</li>
              <li>Emergent structure in Language</li>
              <li>Interactions between Cognitive Linguistics and NLP</li>
              <li>Computational Typology and Morphosyntax</li>
              <li>Evaluation and Interpretability for Low-Resource Languages</li>
            </ul>
          </div>

        </div>

      </div>

      <hr>

      <div id="news">
      <h3 id="scrollspyHeading1">News</h3>
      <div class="entry-content"> 
        <ul id="news-list">
          <li>Invited talks at Stanford University and the University of Nevada at Reno on the topic of "Constructions all the way down: rethinking linguistic generalisation in LLMs". </li>
            <li>Our paper <a href="https://arxiv.org/abs/2409.17005">Models Can and Should Embrace the Communicative Nature of Human-Generated Math</a> was accepted at the NEURIPS 2024 workshop on Mathematical Reasoning and AI!</li>
            <li>Our paper <a href="https://arxiv.org/abs/2408.17437">SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists</a> was accepted to Findings of EMNLP 2024.</li>
            <li>I started my postdoc at UT Austin Linguistics with Kyle Mahowald!</li>
            <li>I gave a keynote at <a href="https://konvens-2024.univie.ac.at/">KONVENS 2024</a> on "Constructions all the way down: Rethinking compositionality in LLMs". The slides are available <a href="files_top/vienna_talk.pdf">here</a>. </li>
            <li>On July 3rd, I successfully defended my PhD thesis "Computational Approaches to Construction Grammar and Morphology" and graduated <i>summa cum laude</i> with a Dr.Phil. in Computational Linguistics! A recording of the 15-minute defence talk is available <a href=videos/defence.mp4>here</a>. </li>
            <li>Invited talks at NYU, Boston University, MIT, UPenn, CMU, and Cornell on the topic of "Finding the Limits of LLMs with Constructions". </li>
            <li>I will be visiting <a href="https://adele.princeton.edu/">Adele Goldberg</a> at <a href="https://www.princeton.edu/">Princeton University</a> for 3 months, starting in March 2024, to work on Construction Grammar and LLMs!</li>
            <li>Three papers accepted to LREC-COLING 2024! <a href="https://arxiv.org/pdf/2403.17748"><i>UCxn: Typologically Informed Annotation of Constructions Atop Universal Dependencies</i></a>, <a href="https://arxiv.org/pdf/2403.17760"><i>Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons</i></a> and <a href="https://arxiv.org/pdf/2403.17856"><i>Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs</i></a>.</li>
            <li>Invited talk at <a href="https://sites.google.com/view/sail-ws-llms/home">SAIL workshop on Fundamental Limits of Large Language Models</a> at the University of Bielefeld. Check out the video <a href="https://www.youtube.com/watch?v=cud7oqj6kik">here</a>!</li>
            <li>Two papers accepted to EMNLP 2023! <a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.15113.pdf"><i>Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model</i></a> and <a href="https://arxiv.org/pdf/2305.12818.pdf"><i>Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs</i></a>.</li>
            <li>Invited talk at the "Poetik, Praxis und Hermeneutik K√ºnstlicher Intelligenz" workshop at the <a href="https://www.fritz-thyssen-stiftung.de/">Fritz Thyssen Foundation</a> in Cologne, on LLMs as the newest battlefield of the Linguistics Wars, featuring opinions by Noam Chomsky, Adele Goldberg, Steve Piantadosi and myself. Slides (in German) are available <a href="https://www.cis.uni-muenchen.de/~weissweiler/files_top/linguistics_wars.pdf">here</a>.</li>
            <li>Invited talk at <a href="https://biu-nlp.github.io/">Bar Ilan University NLP</a></li>
            <li>Two papers accepted to ACL 2023! <a href="https://arxiv.org/pdf/2305.08475.pdf"><i>A Crosslingual Investigation of Conceptualization in 1335 Languages</i></a> and <a href="https://arxiv.org/pdf/2305.15032.pdf"><i>How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives</i></a>.</li>
            <li>I presented <i>The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative</i> at the <a href="https://multiword.org/mwe2023/">19th Workshop on Multiword Expressions</a> at EACL. Check out the <a href=files_top/presentation_eacl_without_video.pdf>slides</a> and <a href=videos/cc.mp4>video</a>!</li>
            <li>Invited talk at <a href="https://www.benjaminroth.net/">Ben Roth's Lab</a> at the University of Vienna</li>
            <li>Invited <a href="https://rycolab.io/talk/weissweiler-mar-01-23/"> talk</a> at <a href="https://rycolab.io/">Ryan Cotterell's Lab</a> at ETH Z√ºrich on construction grammar and how we can use it to formulate new goals for syntactic and semantic probing </li>
            <li>Invited talk at <a href="https://munich-nlp.github.io/">MunichNLP</a> on the joint history of NLP and Linguistics and how we can learn from each other going forward. </li>
            <li>Our paper <i>Construction Grammar Provides Unique Insight into Neural Language Models</i> was accepted to the <a href="https://gurt.georgetown.edu/gurt-2023/cxgsnlp-call-for-papers/">CxGs+NLP Workshop</a> at the <a href="https://gurt.georgetown.edu/gurt-2023/">Georgetown University Round Table 2023</a>! Check it out <a href="https://arxiv.org/abs/2302.02178">here</a>.</li>
            <li>Our paper <i>The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative</i> was accepted to EMNLP 2022! Check it out <a href="https://arxiv.org/abs/2210.13181">here</a>.</li>
            <li>I will be visiting <a href="http://www.cs.cmu.edu/~dmortens/">David Mortensen</a> and <a href="http://www.cs.cmu.edu/~lsl/">Lori Levin</a>'s <a href="https://llab-cmu.github.io/">Linguistics Lab</a> at the <a href="https://www.lti.cs.cmu.edu/">Language Technologies Institute</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a> for 3 months, starting in mid-July 2022, to work on Construction Grammar and NLP!</li>
            <li>Invited at the <a href="https://scads.ai/education/summer-schools/scads-ai-summer-school-2022/">8th International ScaDS Summer School 2022</a> in Leipzig on the joint history of NLP and Linguistics and how we can learn from each other going forward! Here are the <a href="https://scads.ai/education/summer-schools/scads-ai-summer-school-2022/leonie-weissweiler%ef%bf%bc/">abstract</a> and <a href="./files_top/scads.pdf">slides</a></li>
            <li>I presented our paper <i>CaMEL: Case Marker Extraction without Labels</i> üê´ at <a href="https://www.2022.aclweb.org/">ACL 2022</a> in Dublin! Click <a href="files_top/camel.pdf">here</a> for the slides.</li>
            <li>Our paper <i>CaMEL: Case Marker Extraction without Labels</i> üê´ was accepted to ACL 2022! Check it out <a href="https://aclanthology.org/2022.acl-long.377/">here</a>.</li>
          </ul>
          <button id="show-more-btn" class="btn btn-secondary btn-sm mt-3" onclick="toggleNews()">Show More</button>
        </div>
      </div>
      
      <script>
        // Show only the top 10 news items initially
        document.addEventListener("DOMContentLoaded", function() {
          const newsItems = document.querySelectorAll("#news-list li");
          const showMoreBtn = document.getElementById("show-more-btn");
          const maxVisible = 10;
      
          // Hide all items after the first 10
          for (let i = maxVisible; i < newsItems.length; i++) {
            newsItems[i].style.display = "none";
          }
      
          // Toggle function to show/hide news items
          window.toggleNews = function() {
            let isExpanded = showMoreBtn.innerText === "Show Less";
      
            for (let i = maxVisible; i < newsItems.length; i++) {
              newsItems[i].style.display = isExpanded ? "none" : "list-item";
            }
            showMoreBtn.innerText = isExpanded ? "Show More" : "Show Less";
          };
        });
      </script>
      <hr>

      <div id="publications">

        <h3>Selected Publications</h3>
        
        <i>For a full list of publications, please see my <a href="https://scholar.google.com/citations?user=o4fK4n4AAAAJ&hl=en">Google Scholar profile</a>.</i>

        <br>
        <br>
        Jaap Jumelet, <b>Leonie Weissweiler</b>, Joakim Nivre, Arianna Bisazza (2025). <i>MultiBLiMP 1.0: A massively multilingual benchmark of linguistic minimal pairs</i>. Transactions of the Association for Computational Linguistics (TACL), <i>to appear</i>. (<b>TACL</b>)<br>
          <a id="multiblimp_abstract_button" data-bs-toggle="collapse" href="#multiblimp_abstract" role="button" aria-expanded="false" aria-controls="multiblimp_abstract">Abstract</a>
          <a href="https://arxiv.org/pdf/2504.02768" target="_blank">PDF</a>
          <br>
          <div class="collapse" id="multiblimp_abstract" style="padding-top: 20px">
            <div class="card card-body">
              <small>We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages.</small>
            </div>
          </div>

        <br>

        Shijia Zhou, <b>Leonie Weissweiler</b>, Taiqi He, Hinrich Sch√ºtze, David Mortensen, Lori Levin (2024). <i>Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons</i>. Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. (<b>LREC-COLING</b>)<br>
        <a id="so_that_abstract_button" data-bs-toggle="collapse" href="#so_that_abstract" role="button" aria-expanded="false" aria-controls="so_that_abstract">Abstract</a>
        <a href="https://aclanthology.org/2024.lrec-main.336v2.pdf" target="_blank">PDF</a>
        <br>
        <div class="collapse" id="so_that_abstract" style="padding-top: 20px">
          <div class="card card-body">
            <small>In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.</small>
          </div>
        </div>

        <br>

        <b>Leonie Weissweiler</b>,  Abdullatif K√∂ksal, Hinrich Sch√ºtze (2024). <i>Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena</i>. arXiv preprint. (<b>arXiv</b>)<br>
        <a id="cmc_abstract_button" data-bs-toggle="collapse" href="#cmc_abstract" role="button" aria-expanded="false" aria-controls="cmc_abstract">Abstract</a>
        <a href="https://arxiv.org/pdf/2403.06965" target="_blank">PDF</a>
        <br>
        <div class="collapse" id="cmc_abstract" style="padding-top: 20px">
          <div class="card card-body">
            <small>Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale. We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus. We find that all models struggle with understanding the motion component that the CMC adds to a sentence.</small>
          </div>
        </div>

        <br>

        <b>Leonie Weissweiler*</b>, Valentin Hofmann*, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schuetze, Kemal Oflazer, David Mortensen (2023). <i>Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model</i>. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. (<b>EMNLP</b>)<br>
          <a id="wug_abstract_button" data-bs-toggle="collapse" href="#wug_abstract" role="button" aria-expanded="false" aria-controls="wug_abstract">Abstract</a>
          <a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.15113.pdf" target="_blank">PDF</a>
          <br>
          <div class="collapse" id="wug_abstract" style="padding-top: 20px">
            <div class="card card-body">
              <small>Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.</small>
            </div>
          </div>

          <br>


          <b>Leonie Weissweiler</b>, Taiqi He, Naoki Otani, David R. Mortensen, Lori Levin, Hinrich Sch√ºtze (2023). <i>Construction Grammar Provides Unique Insight into Neural Language Models</i>. Proceedings of the First International Workshop on Construction Grammars and NLP (<b>CxGs+NLP, GURT/SyntaxFest 2023</b>), pages 85‚Äì95, Washington, D.C.. Association for Computational Linguistics. <br>
          <a id="position_abstract_button" data-bs-toggle="collapse" href="#position_abstract" role="button" aria-expanded="false" aria-controls="position_abstract">Abstract</a>
          <a href="https://aclanthology.org/2023.cxgsnlp-1.10.pdf" target="_blank">PDF</a>
          <br>
          <div class="collapse" id="position_abstract" style="padding-top: 20px">
            <div class="card card-body">
              <small>Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.</small>
            </div>
          </div>
          
          <br>

          <b>Leonie Weissweiler</b>, Valentin Hofmann, Abdullatif K√∂ksal, Hinrich Sch√ºtze (2022). <i>The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative</i> . Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10859‚Äì10882, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. (<b>EMNLP</b>) <br>
          <a id="cc_abstract_button" data-bs-toggle="collapse" href="#cc_abstract" role="button" aria-expanded="false" aria-controls="cc_abstract">Abstract</a>
          <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.746.pdf" target="_blank">PDF</a>
          <a href="https://github.com/LeonieWeissweiler/ComparativeCorrelative" target="_blank">Source code on Github <img src="util/images/octocat.png" height="12"></a>
          <br>
          <div class="collapse" id="cc_abstract" style="padding-top: 20px">
            <div class="card card-body">
              <small>Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. As a first step towards assessing the compatibility of CxG with the syntactic and semantic knowledge demonstrated by state-of-the-art pretrained language models (PLMs), we present an investigation of their capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC). We conduct experiments examining the classification accuracy of a syntactic probe on the one hand and the models' behaviour in a semantic application task on the other, with BERT, RoBERTa, and DeBERTa as the example PLMs. Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge.</small>
            </div>
          </div>

          
        <hr>

      </div>

      <div>
      <h3>Talks</h3>
      <br>

      <div class="container">

      <div class="row">
      <div class="col-md-4">
        <iframe width="300" height="169" src="https://www.youtube.com/embed/BpujjPe_lYU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>
                  <div class="col-md-8">
            <p>Computational Approaches to Construction Grammar, <i>dissertation defence talk at LMU Munich, 03.07.2024.</i></p>
          </div>
      </div>
      <br>
      <div class="row">
      <div class="col-md-4">
        <iframe width="300" height="169" src="https://www.youtube.com/embed/RjExD1DgDKU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>
                  <div class="col-md-8">
            <p>Finding the Limits of LLMs with Constructions, <i>talk given at NYU, Boston University, MIT, UPenn, CMU, and Cornell.</i></p>
          </div>
      </div>
      <br>
      <div class="row">
          <div class="col-md-4">
          <iframe width="300" height="169" src="https://www.youtube.com/embed/cud7oqj6kik?si=VJe7c15R93wPyFQt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>
            <div class="col-md-8">
            <p>Testing the Limits of LLMs with Construction Grammar, <i>talk given at Bielefeld University.</i></p>
          </div>
          </div>
          <br>
        <div class="row">
          <div class="col-md-4">
            <iframe width="300" height="169" src="https://www.youtube.com/embed/3TQCrsXTY0A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>
            <div class="col-md-8">
            <p>Everything is a Construction: New Goals for Syntactic and Semantic Probing, <i>talk given at ETH Z√ºrich, University of Vienna, and Bar Ilan University.</i></p>
          </div>
          </div>
          <br>
        <div class="row">
        <div class="col-md-4">
          <iframe width="300" height="169" src="https://www.youtube.com/embed/4ae20ZNs6fc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <!-- <iframe width="300" height="" src="" title="video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
                </div>
          <div class="col-md-8">
          <p>Construction Grammar Provides Unique Insight into Neural Language Models, <i> talk given at <a href="https://gurt.georgetown.edu/gurt-2023/cxgsnlp-call-for-papers/">CxGs+NLP Workshop</a> at the <a href="https://gurt.georgetown.edu/gurt-2023/">GURT 2023</a></i></p>
        </div>
        </div>
        <br>
        <div class="row">
        <div class="col-md-4">
          <iframe width="300" height="169" src="https://www.youtube.com/embed/98CeTp7pxeU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <!-- <iframe width="300" height="" src="" title="video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
                </div>
          <div class="col-md-8">
          <p>The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative, <i>video recorded for EMNLP 2022</i></p>
        </div>
        </div>
        <br>
        <div class="row">
        <div class="col-md-4">
        <iframe width="300" height="169" src="https://www.youtube.com/embed/tBQKswYlQNs?si=9wyzLzbVMQSFNnCg" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
          <div class="col-md-8">
          <p>The Past, Present, and Future of NLP from a Linguistic Perspective, <i>talk given at <a href="https://scads.ai/education/summer-schools/scads-ai-summer-school-2022/">8th International ScaDS Summer School 2022</a> and <a href="https://munich-nlp.github.io/">MunichNLP</a> </i></p>
        </div>
        </div>

    
  </div>
</div>
</div>
<hr>
<br>

      <h3>Thesis supervision</h3>
        <div class="entry-content">
          <ul>
            <li>Language Models and Construction Grammar, MSc thesis, <i>09/23-02/24</i></li>
            <li>Introducing Syntactic Inductive Biases in Language Model Pre-Training, Msc thesis, <i>09/22-01/23</i></li>
            <li>Language Model Compression with Teaching Assistants, Msc thesis, <i>01-10/2022</i>, <a href="https://xinpeng-wang.github.io/">Xinpeng Wang</a>, now at <a href="https://mainlp.github.io/">MaiNLP</a></li>
            <li>Creating a Multilingual Gold Standard for Case Marker Extraction, BSc thesis, <i>03-06/2022</i></li>
            <li>Unsupervised Induction of Construction Grammars, BSc thesis, <i>03-06/2022</i></li>
            <li>Investigating Emergent Linguistic Structure in BERT using Attention Patterns, MSc thesis, <i>03-07/2021</i></li>
          </ul>
        </div>

      <br>

    </div>
  </div>
  <!--==================J S FOR MOVING ABSTRACTS============================-
        ->





		<!-- =============== BOOTSTRAP NECESSITES ================= -->
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <!-- Include all compiled plugins (below), or include individual files
        as needed -->
</body>

</html>